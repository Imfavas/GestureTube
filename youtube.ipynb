{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imported necessory libraires\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import datetime\n",
    "import math\n",
    "import pyautogui\n",
    "\n",
    "class Control: # created a class named control \n",
    "    def __init__(self): # initialised the calss with some necessory variables\n",
    "        self.mp_hands = mp.solutions.hands\n",
    "        self.mp_draw = mp.solutions.drawing_utils\n",
    "        self.hands = self.mp_hands.Hands(max_num_hands=2, min_detection_confidence=0.7)\n",
    "\n",
    "        self.mp_face_detection = mp.solutions.face_detection\n",
    "        self.face_detection = self.mp_face_detection.FaceDetection(min_detection_confidence=0.7)\n",
    "\n",
    "        self.video = cv2.VideoCapture(0) \n",
    "        self.then = datetime.datetime.now()  # created a variable named then to implement latency int the actions\n",
    "\n",
    "    def detect_gesture(self, landmarks):     # the landmarks will be passed from run function \n",
    "        if len(landmarks) != 21:             # checked if all 21 landmarks are present \n",
    "            return None\n",
    "\n",
    "        lmlist = [[i, pt[0], pt[1]] for i, pt in enumerate(landmarks)]   # This code is a list comprehension to collect id,cx,cy\n",
    "        fingerlist = []                                                  # we initialised a finger list to append on off fingers\n",
    "        tip_ids = [4, 8, 12, 16, 20]                                     # Landmark IDs for fingertips\n",
    "\n",
    "        # Determine if it's a right or left hand.\n",
    "        is_right_hand = lmlist[9][1] < lmlist[17][1]  # True if right hand, False if left hand\n",
    "        # Thumb check based on hand type\n",
    "        if is_right_hand:\n",
    "            fingerlist.append(0 if lmlist[4][1] > lmlist[3][1] else 1)\n",
    "        else:\n",
    "            fingerlist.append(0 if lmlist[4][1] < lmlist[3][1] else 1)\n",
    "\n",
    "        # Appending Other fingers\n",
    "        for i in range(1, 5):\n",
    "            fingerlist.append(1 if lmlist[tip_ids[i]][2] < lmlist[tip_ids[i] - 2][2] else 0)\n",
    "\n",
    "        return fingerlist, is_right_hand  \n",
    "    '''\n",
    "    - Fingerlist will return the list of 0 and 1 where 0 representing finger closed and 1 representing finger open\n",
    "    - is_right_hand returns a true value if the hand shown is right else a false value\n",
    "    '''\n",
    "\n",
    "    def control_media_right_hand(self, gesture): # function containing which key needs to be pressed if hand is right\n",
    "        if gesture == [0, 1, 0, 0, 1]:\n",
    "            pyautogui.press(\"volumemute\", interval=0.8)\n",
    "        elif gesture == [0, 1, 0, 0, 0]:\n",
    "            pyautogui.hotkey(\"shift\", \"<\", interval=0.5)  \n",
    "        elif gesture == [0, 0, 0, 0, 1]:\n",
    "            pyautogui.hotkey(\"shift\", \">\", interval=0.5)  \n",
    "        elif gesture == [1, 1, 1, 1, 1]:\n",
    "            pyautogui.press(\"space\", interval=0.8)  \n",
    "        elif gesture == [0, 0, 1, 1, 1]:\n",
    "            pyautogui.scroll(10)\n",
    "        elif gesture == [0, 1, 1, 1, 0]:\n",
    "            pyautogui.press(\"volumeup\", interval=0.3)\n",
    "        elif gesture == [0, 1, 1, 0, 0]:\n",
    "            pyautogui.press(\"volumedown\", interval=0.3)\n",
    "        elif gesture == [0, 0, 0, 0, 0]:\n",
    "            pyautogui.press(\"q\")\n",
    "        elif gesture == [1, 0, 0, 0, 0]:\n",
    "            pyautogui.hotkey(\"shift\",\"p\", interval=0.5)\n",
    "\n",
    "    def control_media_left_hand(self, gesture): # function containing which key needs to be pressed if hand is left\n",
    "        if gesture == [0, 0, 1, 1, 1]:\n",
    "            pyautogui.scroll(-10)  # Scroll down\n",
    "        elif gesture == [1, 1, 1, 1, 1]:\n",
    "            pyautogui.press(\"esc\") \n",
    "        elif gesture == [1,1,0,0,0] :\n",
    "            pyautogui.press(\"f\",interval=0.8)\n",
    "        elif gesture == [0, 0, 0, 0, 1]:\n",
    "            pyautogui.hotkey(\"i\", interval=0.5)\n",
    "        elif gesture == [1, 0, 0, 0, 0]:\n",
    "            pyautogui.hotkey(\"shift\",\"n\", interval=0.5) \n",
    "            \n",
    "\n",
    "    def detect_face_tilt(self, frame, face_results):\n",
    "        \"\"\"Detects head tilt based on face detection.\"\"\"\n",
    "        if not face_results.detections:\n",
    "            return\n",
    "\n",
    "        for detection in face_results.detections:\n",
    "            h, w, _ = frame.shape\n",
    "            self.mp_draw.draw_detection(frame, detection)\n",
    "\n",
    "            # Extract eye keypoints\n",
    "            right_eye = detection.location_data.relative_keypoints[0]\n",
    "            left_eye = detection.location_data.relative_keypoints[1]\n",
    "\n",
    "            right_eye_coord = (int(right_eye.x * w), int(right_eye.y * h))\n",
    "            left_eye_coord = (int(left_eye.x * w), int(left_eye.y * h))            \n",
    "            # The code till here is to find the x,y points of right eye and left eye\n",
    "\n",
    "            cv2.circle(frame, right_eye_coord, 3, (0, 255, 255), -1)\n",
    "            cv2.circle(frame, left_eye_coord, 3, (0, 255, 255), -1)\n",
    "\n",
    "            # Compute the angle between eyes\n",
    "            dx = right_eye_coord[0] - left_eye_coord[0]\n",
    "            dy = right_eye_coord[1] - left_eye_coord[1]\n",
    "            angle = math.degrees(math.atan2(dy, dx))\n",
    "\n",
    "            # Normalize the angle to [-90, 90]\n",
    "            angle = angle + 180 if angle < -90 else angle - 180 if angle > 90 else angle # Normalisation code to calculate the angle\n",
    "\n",
    "            cv2.putText(frame, f\"Head Angle: {int(angle)}\", (50, 100),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "            tilt_threshold = 23                                                          # we use tilt threshold in deciding after which angle the key to be pressed                                          \n",
    "            now = datetime.datetime.now()                                                # used to get time for now to calculate total seconds\n",
    "            tot = now - self.then                                                        # Used to delay the key presses, we can set the key to be pressed after 1 second delay\n",
    "            tot_int = int(tot.total_seconds())\n",
    "\n",
    "            if angle < -tilt_threshold and tot_int >= 1:\n",
    "                pyautogui.press(\"j\")  # Previous slide\n",
    "                self.then = datetime.datetime.now()\n",
    "            elif angle > tilt_threshold and tot_int >= 1:\n",
    "                pyautogui.press(\"l\")  # Next slide\n",
    "                self.then = datetime.datetime.now()\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Main loop for detecting hand gestures and face tilt.\"\"\"\n",
    "        while True:\n",
    "            suc, frame = self.video.read()\n",
    "            if not suc:\n",
    "                break\n",
    "\n",
    "            frame = cv2.flip(frame, 1)  # Flip frame horizontally\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            hand_results = self.hands.process(rgb_frame)\n",
    "            face_results = self.face_detection.process(rgb_frame)\n",
    "\n",
    "            if hand_results.multi_hand_landmarks:\n",
    "                for i, hand_landmarks in enumerate(hand_results.multi_hand_landmarks):\n",
    "                    h, w, _ = frame.shape\n",
    "                    landmarks = [(int(lm.x * w), int(lm.y * h)) for lm in hand_landmarks.landmark] \n",
    "                    # we append the landmarks into this list this list is then passed to detect_gesture\n",
    "\n",
    "                    self.mp_draw.draw_landmarks(frame, hand_landmarks, self.mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                    gesture, is_right_hand = self.detect_gesture(landmarks)   \n",
    "                    # we call the detect_gesture here from there we get if the hand is right hand or not\n",
    "                    # and also the gesture list  \n",
    "                    hand_label = \"Right Hand\" if is_right_hand else \"Left Hand\" # outputs based on which hand is it\n",
    "\n",
    "                    cv2.putText(frame, f\"{hand_label} Gesture: {gesture}\", (50, 50 + i * 30),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "                    if is_right_hand:\n",
    "                        self.control_media_right_hand(gesture) # if the hand is right then the gesture list is given to control media right hand\n",
    "                    else:\n",
    "                        self.control_media_left_hand(gesture)  # if the hand is left then the gesture list is passed to control media left hand\n",
    "\n",
    "            self.detect_face_tilt(frame, face_results)\n",
    "\n",
    "            cv2.imshow(\"YouTube Gesture Control\", frame)\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "        self.video.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Control()\n",
    "model.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Guide\n",
    "- **Right hand**\n",
    "    -  rock symbol = mute\n",
    "    -  open hand = pause/unpause\n",
    "    -  index finger = to reduce the playback speed\n",
    "    -  pinky = to increace the playback speed\n",
    "    -  combination of last 3 fingers = scroll up\n",
    "    -  combination of middle and index finger = volume down\n",
    "    -  combination of middle,ring,and index = volume up\n",
    "    -  thump open = go to previous video\n",
    "- **Left hand**\n",
    "    -  combination of last 3 fingers = scroll up\n",
    "    -  open hand = escape key\n",
    "    -  L gesture = full screen\n",
    "    -  pinkey finger = minimise\n",
    "    -  thumb open = go to next video\n",
    "\n",
    "- **Head**\n",
    "    -  tilt right to skip 10 seconds\n",
    "    -  tilt left to go back 10 seconds "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Scope:\n",
    "- As of now the project is only focusing on Youtube control, we can create different functions here for other video players\n",
    "- We can add more gestures and more controlls acording to the features and keyboard shortcuts available"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
